{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Data Preparation Workshop\n",
    "September 2019\n",
    "\n",
    "* ## 1. (Nominal) Categorical Variables -- Distinct values but not *ordered*\n",
    "  * You will convert categorical variables into usable predictive variables.\n",
    "* ## 2. Imputation -- Filling in Missing Variables\n",
    "  * You will see how to fill in missing variables with the Titanic dataset\n",
    "  * Optionally, you will be able to run CatBoost for data which has not had significant missing and categorical pre-processing.\n",
    "* ## 3. Cyclical Variables -- Interesting challenges for models which assume ordered, continuous variables\n",
    "  * You will have an opportunity to model a cyclical variable as two continuous variables.\n",
    "   \n",
    "Note: a companion workbook on data preparation techniques discusses concepts in greater detail.  This workbook is for a workshop to gain hands-on experience with related python code.\n",
    "\n",
    "Expectation is some familiarity with programming or pandas/python.  *Feel free to work with a partner if you are unfamiliar with python and jupyter.*  There are a couple places where pandas syntax may be required and the comments are intended to guide you through this.\n",
    "\n",
    "Anaconda installation: https://docs.google.com/document/d/11N6IAy3NeD6eMv5jxq5h9jOW_dVTfTs8txHsELzCx9w/edit?usp=sharing\n",
    "There is a small section using CatBoost, and installation instructions for it are at the end of the Anaconda document above.  You may decide to skip the CatBoost tutorial section if you have difficulty with installation.\n",
    "\n",
    "All sections are to be \"run\" in sequence.  Additional instructions are provided when work is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge, LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn\n",
    "import catboost\n",
    "print('Checking installed versions:')\n",
    "print(f'pandas version installed {pd.__version__} -- should be at least 0.24.2')\n",
    "print(f'sklearn version installed {sklearn.__version__} -- should be at least 0.21.2')\n",
    "print(f'catboost version installed {catboost.__version__} -- should be at least 0.17.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic Data Set \n",
    "\n",
    "On April 14, 1912 the Titanic hit an iceberg and it sank on April 15.  An estimated 710 passengers were saved and 1,514 died.  A dataset has been derived from this disaster (data circa 1999) and used by universities and adopted by Kaggle.\n",
    "\n",
    "* Dataset is in the data/titanic subdirectory of this project\n",
    "* Observations are per passenger.  PassengerId exists but will not be used as it is arbitrary.\n",
    "\n",
    "**survival**:\tDid passenger survive the Titanic crash? (0 = No, 1 = Yes)\n",
    "\n",
    "**pclass**:\tTicket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "\n",
    "**sex**:\tSex of passenger\n",
    "\n",
    "**Age**:\tAge in years\n",
    "\n",
    "**sibsp**:\t# of siblings / spouses aboard the Titanic (see note)\n",
    "\n",
    "**parch**:\t# of parents / children aboard the Titanic (see note)\n",
    "\n",
    "**ticket**:\tTicket number\n",
    "\n",
    "**fare**:\tPassenger fare (GBP)\n",
    "\n",
    "**cabin**:\tCabin number\n",
    "\n",
    "**embarked**:\tPort of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "\n",
    "More details about the data set are found at: http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf.  From that note:\n",
    "\n",
    "\"With respect to the family relation variables (i.e. sibsp and parch) some relations were\n",
    "ignored. The following are the definitions used for **sibsp** and **parch**.\n",
    "Sibling: Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\n",
    "Spouse: Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances\n",
    "Ignored)\n",
    "Parent: Mother or Father of Passenger Aboard Titanic\n",
    "Child: Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n",
    "Other family relatives excluded from this study include cousins, nephews/nieces,\n",
    "aunts/uncles, and in-laws. Some children travelled only with a nanny, therefore parch=0\n",
    "for them. As well, some travelled with very close friends or neighbors in a village,\n",
    "however, the definitions do not support such relations.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading titanic from this repo.  The data set has been split into train and test; we will work with train here.\n",
    "train_df = pd.read_csv('data/titanic/train.csv')\n",
    "train_df.info()\n",
    "# Note that there are 891 observations for PassengerId.\n",
    "# Columns with Nulls will report fewer than 891 observations.  These have to be handled as missing data\n",
    "\n",
    "# Note: titanic is also in the catboost repo, but not used in case a student has not installed catboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent Variable: Whether passenger survived the Titanic crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use value_counts() to get distribution:\n",
    "train_df.Survived.value_counts()\n",
    "\n",
    "# Notice how Survived has 891 non-null values, and they are 0 and 1\n",
    "# Remember this frequency distribution syntax for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation and Data Processing -- Handle Missing Values: Fewer than 891 observations\n",
    "\n",
    "* Embarked\n",
    "* Age\n",
    "* Cabin\n",
    "\n",
    "### Embarked: What do we know about the two passengers not listed as embarked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas syntax for checking NULLs.  Notice how pandas uses something like a SQL where clause in square brackets.\n",
    "train_df[train_df.Embarked.isnull()==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What action should we take for observations without an embarkation port?\n",
    "\n",
    "* Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of passengers without a Null Embarked\n",
    "train_df[train_df.Embarked.isnull()==True].PassengerId.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Use pandas.get_dummies() to one hot encode Embarked ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calls get_dummies and concatenates this to the data set\n",
    "train_df = pd.concat([train_df, pd.get_dummies(train_df.Embarked, prefix='Embarked')], axis=1)\n",
    "\n",
    "# note new columns added to dataframe\n",
    "train_df.info()\n",
    "\n",
    "# note how new columns relate to the Embarked column\n",
    "sample_table = train_df[train_df.Embarked.isnull()==False].head(5)\n",
    "display(HTML(sample_table.to_html()))  # fancy syntax for pretty printing tables in jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Age.hist(figsize=(12,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SimpleImputer to create a sample column -- illustrating what happens if we impute age as mean when missing\n",
    "\n",
    "* This is just to demonstrate how the code works, we will use a more sophisticated imputation of Age below\n",
    "* One could trivially do these steps without the class \"SimpleImputer\" but it could prove useful at scale (multiple variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(train_df[['Age']])\n",
    "print(imp_mean.transform(train_df[['Age']])[:20])\n",
    "# Note how all imputed values are about 30 years of age.  Feel free to try another strategy such as median or most_frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age: Let's impute it from other variables rather than simply using mean\n",
    "\n",
    "* We will first convert other object variables to features\n",
    "* Then we will impute Age once other variables are ready\n",
    "\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cabin\n",
    "\n",
    "* Cabin does not look that useful; maybe the first character means something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Write code to show how many records are missing Cabin.  (See above for example)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Look at the some sample values for Cabin \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like the first character in Cabin is the cabin deck.  \n",
    "# Here you create a new variable based on the first character\n",
    "train_df['CabinDeck'] = train_df.Cabin.str.slice(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Print out the frequency distribution for CabinDeck (see above for examples)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that CabinDeck is missing as often as Cabin\n",
    "train_df[train_df.CabinDeck.isnull()==True].PassengerId.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Experiment: What do you want to do about Cabins?\n",
    "\n",
    "* We have Cabins for less than half of the variables?\n",
    "* Even when we do have cabins, the T level and G level have few observations, and the T level is perhaps wrong \n",
    "(since it would be the Tank Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine sample data\n",
    "train_df[train_df.CabinDeck=='T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine sample data\n",
    "train_df[train_df.CabinDeck=='G']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thought Experiments\n",
    "\n",
    "* Do you want to drop the column Cabin because so much information is missing?\n",
    "* Would you impute the column?\n",
    "* Or we could use the CabinDeck and keep a category for \"no information\", which might tell us something about the class or person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, pd.get_dummies(train_df.CabinDeck, prefix='CabinDeck')], axis=1)\n",
    "\n",
    "# resulting columns:\n",
    "print(train_df.info())\n",
    "\n",
    "# resulting data:\n",
    "sample_table = train_df[train_df.CabinDeck.isnull()==False].head(5)\n",
    "display(HTML(sample_table.to_html()))\n",
    "# notice how dummy columns correspond to the values in CabinDeck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the CabinDeck_T variable since T does not seem like a proper cabin (by removing it we treat it similar to passengers without cabin deck)\n",
    "train_df.drop(columns=['CabinDeck_T'], inplace=True)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables - What Other Variables Should be Considered?\n",
    "\n",
    "* Variables type object are all strings.  What will you do with them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes[train_df.dtypes=='object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Sex.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach #1: Pandas get_dummies - one hot encode Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Use pandas get_dummies to create new variables (see above example)\n",
    "# Verify that the values appear sensible.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach #2: scikit-learn one hot encoder -- illustrating another approach\n",
    "\n",
    "* This approach demonstrates other methods to accomplish the same objective\n",
    "* We will print out results but not use this method\n",
    "\n",
    "When would you use these two approaches?\n",
    "* pd.get_dummies - quick and useful in pandas\n",
    "* OneHotEncoder - use when in numpy and not in pandas, simply using arrays not dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(train_df.Sex.to_frame())\n",
    "print(enc.categories_)\n",
    "transform_example = enc.transform(train_df.Sex.to_frame())\n",
    "# Note: with one hot encoding you will have an array similar to above, but you have to do the work to put it back in a dataframe\n",
    "transform_example[:10].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ticket\n",
    "\n",
    "Does this look useful to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Ticket.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review other quantitative variales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: describe Fare, and determine whether the data appears sensible\n",
    "# \n",
    "# Thought experiment -- Do you wish to modify any of the Fares?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pclass and Parch variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Pclass.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parents/children aboard ship\n",
    "train_df.Parch.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does one have questions about accuracy of this data?  7 people had 8 spouses / siblings aboard ship?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way to explore the data might be to look at the person who had 6 children\n",
    "train_df[train_df.Parch==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of siblings or spouses aboard ship.\n",
    "train_df.SibSp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Investigate passengers with 5 or 8 Siblings or Spouses. Is this data credible?\n",
    "#\n",
    "# if behind, please skip to next step...\n",
    "#\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone Check - Where Are We?\n",
    "\n",
    "* Plan to exclude dependent variable:\n",
    "  * Survived\n",
    "\n",
    "* Do not plan to use:\n",
    "  * PassengerId    \n",
    "  * Ticket\n",
    "  * Name -- not investigated yet\n",
    "  \n",
    "* Features created from raw variables:\n",
    "  * male/female: Replace Sex.  In some instances use one\n",
    "  * CabinDeck: instead of Cabin\n",
    "  * Embarked_S, Embarked_Q, Embarked_C: rather than Embarked\n",
    "  \n",
    "* Numeric raw variables available:\n",
    "  * SibSp\n",
    "  * Fare\n",
    "  * Pclass\n",
    "  * Parch\n",
    "  \n",
    "* Now we plan to impute **Age** which has some missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return to Imputation \n",
    "### Create a new column called AgeImputedPred: Predict the age from all other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine some sample data with Age missing.\n",
    "train_df[train_df.Age.isnull()==True].head(6)\n",
    "# ... especially interesting that titles like \"Mr.\", \"Mrs.\", \"Miss\", \"Master\" might suggest age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax to pick out a few titles\n",
    "display(HTML(train_df[train_df.Name.str.contains('Miss')].head(3).to_html()))\n",
    "display(HTML(train_df[train_df.Name.str.contains('Master')].head(3).to_html()))\n",
    "display(HTML(train_df[train_df.Name.str.contains('Mr.')].head(3).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could we use title of name for averaging of ages?\n",
    "known_titles = ['Mr.', 'Mrs.', 'Miss', 'Master', 'Rev.', 'Col.', 'Mlle.', 'Dr.', 'Ms.', 'Major', 'Mme.', 'Don.', 'Capt.', 'Jonkheer', 'Countess']\n",
    "\n",
    "for title in known_titles:\n",
    "    has_title = train_df[train_df.Name.str.contains(title)]\n",
    "    print(f'{title}: Count - {has_title.Age.count()}; Mean Age - {np.round(has_title.Age.mean(),1)}')\n",
    "    missing_age = has_title[has_title.Age.isnull()==True]\n",
    "    print(f'  # missing age is: {missing_age.PassengerId.count()}')\n",
    "print(train_df[train_df.Name.str.contains('|'.join(known_titles))==False].Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Steps: For Students With Lots of Time...\n",
    "\n",
    "* It would be helpful to create a categorical variable for each of the titles in the data set.\n",
    "* Then as you go through the code below which imputes age, add in these category variables as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for optional exercise for advanced staudents\n",
    "#\n",
    "# skip if you do not have time\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a new column called AgeImputedPred: Predict the age from all other variables\n",
    "#\n",
    "col_list = ['Age', 'Sex_male', 'Sex_female', 'Embarked_S', 'Embarked_Q', 'Embarked_C', 'SibSp', 'Fare', 'Pclass', 'Parch', 'CabinDeck_A', 'CabinDeck_B', 'CabinDeck_C', 'CabinDeck_D', \n",
    "            'CabinDeck_E', 'CabinDeck_F', 'CabinDeck_G']\n",
    "print(train_df[col_list].head(3))\n",
    "# after cleaning up and having cat variables edit and do this:\n",
    "iterative_imp = IterativeImputer(random_state=0, estimator=BayesianRidge())\n",
    "iterative_imp.fit(train_df[col_list])\n",
    "result = iterative_imp.transform(train_df[col_list])\n",
    "print(type(result))\n",
    "print(result[:6, 0])\n",
    "train_df['Age_Imputed'] = result[: ,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Age_Imputed.describe()\n",
    "# Notice anything strange in the imputed ages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts of Ages: How do Imputed Ages Compare with Acutal Ages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train_df[train_df.Age.isnull()==False].Age_Imputed.hist(figsize=(14,6))\n",
    "plt.suptitle('Original Values of Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train_df[train_df.Age.isnull()==True].Age_Imputed.hist(figsize=(14,6))\n",
    "plt.suptitle('Imputed Values of Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Age_Imputed.hist(figsize=(14,6))\n",
    "plt.suptitle('Passenger Age (Imputed and Non-Imputed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One might be concerned with ages below zero.  They seem to be a set of children from the same large family.\n",
    "train_df[train_df.Age_Imputed<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Exercise: Clean up the negative ages\n",
    "#\n",
    "# This assumes pandas experience with loc[].\n",
    "#\n",
    "#\n",
    "# One could substitute a slightly better age, perhaps 1, for these children.  Let's assume they are septuplets!?\n",
    "#\n",
    "# Using loc() function set the imputed age to 1.  Modify this line filling in the loc parameters:\n",
    "train_df.loc[<>, <>] = 1\n",
    "\n",
    "\n",
    "\n",
    "# Verify no negative ages left\n",
    "train_df[train_df.Age_Imputed<0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: again plot the Age_Imputed histogram (.hist function) after making the correction above\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Different Classification Models with these Variables\n",
    "\n",
    "* What do you notice about model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df[['Survived']]\n",
    "all_cols = set(train_df.columns.tolist())\n",
    "# the original categorical columns are marked unusable here, so the poss_cols includes new columns and quantitative columns\n",
    "unusable_cols = set(['PassengerId', 'Survived', 'Name', 'Sex', 'Age', 'Cabin', 'Embarked', 'female', 'CabinDeck', 'Ticket'])\n",
    "poss_cols = all_cols - unusable_cols\n",
    "poss_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Passenger class and gender\n",
    "\n",
    "* LogisticRegression returns mean accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limited model: passenger class and gender only\n",
    "var = ['Pclass', 'Sex_male']\n",
    "X = train_df[var]\n",
    "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(X, np.ravel(y))\n",
    "scores = cross_val_score(model, X, np.ravel(y), cv=3)\n",
    "print(f'Accuracy scores {scores}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: All prepared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in all columns only slightly improves model over gender and class\n",
    "X = train_df[poss_cols]\n",
    "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr', max_iter=5000).fit(X, np.ravel(y))\n",
    "scores = cross_val_score(model, X, np.ravel(y), cv=3)\n",
    "print(f'Accuracy scores {scores}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - CatBoost - Optional Section\n",
    "\n",
    "* If you run out of time to install CatBoost for this tutorial you could skip this section.\n",
    "* This algorithm will take 3 minutes to execute with modest computing power\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "from catboost import Pool, CatBoostClassifier, cv\n",
    "# must have catboost instaled to run this section\n",
    "\n",
    "cat_train_df = pd.read_csv('data/titanic/train.csv')\n",
    "cat_y = cat_train_df[['Survived']]\n",
    "print(cat_train_df.columns)\n",
    "cat_train_df.drop(columns=['PassengerId', 'Survived'], inplace=True)\n",
    "cat_x = cat_train_df\n",
    "cate_features_index = np.where(cat_x.dtypes != float)[0]\n",
    "cat_train_df.fillna(-999,inplace=True)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(cat_x, cat_y, train_size=.85, random_state=1234)\n",
    "model = CatBoostClassifier(custom_loss=['Accuracy'],random_seed=42,logging_level='Silent', iterations=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next step takes sometime to run, and metric_period determines how often it will output.  The * to the left of the cell will remain while running.\n",
    "\n",
    "* The plot only seems to work in jupyter notebook not jupyter lab; possibly because the widgetsnbextension is not turned on in lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on the \"Accuracy\" rather than \"LogLoss\" to get a sense of accuracy\n",
    "model.fit(xtrain, ytrain,cat_features=cate_features_index,eval_set=(xtest, ytest), plot=True,  metric_period=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative code with cross validation - advanced users could uncomment and work with this\n",
    "#\n",
    "# model = CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=42)\n",
    "# cv_data = cv(Pool(data=cat_x, label=cat_y, cat_features=cate_features_index), model.get_params(), fold_count=3, metric_period=50, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical Variables - How Can Data Preparation Help with Cyclical Variables?\n",
    "\n",
    "A one dimensional variable such as day of year captures continuous information (progress of human history) as well as cyclic information (seasons or portion of the year).  In this example we show how taking the *sin and cos of the fraction of the cycle* converts the period of the year into two dimensions, allowing similarity to be created for adjacent and seasonal values.\n",
    "\n",
    "First, an example of code and plotting which you can use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code and plotting\n",
    "np.random.seed(42)\n",
    "rand_days = np.append(np.random.randint(0, 365, 25), [11, 254, 284])\n",
    "# print(rand_days)\n",
    "df=pd.DataFrame(rand_days, columns=['day_num'])\n",
    "df['sin_of_date'] = np.sin(2*np.pi*df.day_num/365)\n",
    "df['cos_of_date'] = np.cos(2*np.pi*df.day_num/365)\n",
    "plt = df.plot.scatter('sin_of_date','cos_of_date', figsize=(10, 10))\n",
    "plt.set_title('Day of Year Distributed Across Two Variables')\n",
    "for dname, dnum in (['Jan 11', 11], ['Sep 11', 254], ['Oct 11', 284]):\n",
    "    _ = plt.text(df[df.day_num==dnum].sin_of_date.values[0]+.02, df[df.day_num==dnum].cos_of_date.values[0], dname)\n",
    "\n",
    "_ = plt.text (-1, 1, 'Temperature related to cos(date)')\n",
    "_ = plt.text (-1, -1, 'Business seasons to sin(date)')\n",
    "print('Similar day next year will have same values on these two features.')\n",
    "print('You may also elect to have day in human history as a third variable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workshop Activities:\n",
    "\n",
    "* Simulate Cyclic Variable\n",
    "* Perform statistical analysis several ways\n",
    "  * Treating variable as dummy\n",
    "  * Treating variable as ordinal\n",
    "  * Treating variable as cyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate events occurring at various hours of the day\n",
    "\n",
    "# Independent variables are time\n",
    "time_array = np.random.randint(0, 24, size=(2500))\n",
    "\n",
    "# data checks\n",
    "print(time_array)\n",
    "print(min(time_array))\n",
    "print(max(time_array))\n",
    "sim_df = pd.DataFrame(time_array, columns=['time_of_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent variable:\n",
    "# Simulate function where dependent variable is driven by absolute difference in time from 10pm (22) plus a Gaussian normal as a stochastic factor\n",
    "sim_df['hours_before_22'] = np.maximum(0, 22 - sim_df.time_of_day)\n",
    "sim_df['hours_after_22'] = 24 - 22 + sim_df.time_of_day\n",
    "sim_df['distance_from_22'] = sim_df[['hours_before_22', 'hours_after_22']].min(axis=1)\n",
    "display(HTML(sim_df.head(4).to_html()))\n",
    "np.random.seed(seed=42)\n",
    "sim_df['y'] = 20 + np.random.normal(size=len(sim_df))*4 - sim_df.distance_from_22\n",
    "display(HTML(sim_df.head(4).to_html()))\n",
    "sim_df.plot.scatter(x='distance_from_22', y='y', figsize=(12,8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A: Regress on ordinal value\n",
    "\n",
    "* Naive model: we do not know underlying algorithm and our only real independent variable is time of day\n",
    "* print out r^2 scores\n",
    "\n",
    "Simpified code uses cross_val_score method, which performs k-fold cross validation, seemingly equivalent to larger code block:\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "for i, (train, test) in enumerate(kfold.split(X, y)):\n",
    "    model.fit(X.iloc[train,:], y.iloc[train,:])\n",
    "    score = model.score(X.iloc[test,:], y.iloc[test,:])\n",
    "    scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataframes for X and y:\n",
    "y = sim_df[['y']]\n",
    "X = sim_df[['time_of_day']] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive model: treat time_of_day as an interval variable:\n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X, y, cv=3)\n",
    "print(scores)\n",
    "# r^2 score is not that great, below 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B: Treat hour of day as categorical\n",
    "\n",
    "* print out r^2 scores\n",
    "\n",
    "Note: in practice this option would discard information, for example we could not realistically add a categorical variable for every minute.  We are showing it here so you have a benchmark of a model which had complete information with a traditional approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_var are hour of day as dummy variables\n",
    "add_var = pd.get_dummies(X.time_of_day, prefix='hour', drop_first=True)\n",
    "# Add all the columns to the model data\n",
    "X = X.join(add_var)\n",
    "# Drop the original column that was expanded\n",
    "X.drop(columns=['time_of_day'], inplace=True)\n",
    "print(X.head(3))\n",
    "\n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X, y, cv=3)\n",
    "print(scores)\n",
    "\n",
    "# note how the r^2 improved considerably"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C: Treat hour of day as cyclic variable\n",
    "\n",
    "* print out r^2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Define sin_hour and cos_hour per model above\n",
    "sim_df['sin_hour'] = <>\n",
    "sim_df['cos_hour'] = <>\n",
    "X = sim_df[['sin_hour', 'cos_hour']]\n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X, y, cv=3)\n",
    "print(scores)\n",
    "\n",
    "# comparable results without adding a dummy variable for every time period, something which would be infeasible if hours and minutes were added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced users: can you rewrite this simulation using time of day instead of hour of day?  \n",
    "# What results do you find?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
